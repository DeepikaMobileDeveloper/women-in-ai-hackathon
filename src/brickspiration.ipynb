{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VvxT-azylyV",
        "outputId": "1f083009-8f72-412f-e1fd-bffe6efda632"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'women-in-ai-hackathon' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/DeepikaMobileDeveloper/women-in-ai-hackathon.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-VdtTeZE5d9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "data_dir = Path('women-in-ai-hackathon/data')\n",
        "for gz_file in data_dir.glob('*.gz'):\n",
        "   csv_file = gz_file.with_suffix('.csv')\n",
        "   with gzip.open(gz_file, 'rb') as f_in:\n",
        "       with open(csv_file, 'wb') as f_out:\n",
        "           shutil.copyfileobj(f_in, f_out)\n",
        "   print(f'Unzipped: {gz_file.name}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuy2JsmXyxjM",
        "outputId": "812b21b6-ed87-4ded-85ea-eb1012261618"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unzipped: inventory_sets.csv.gz\n",
            "Unzipped: inventories.csv.gz\n",
            "Unzipped: themes.csv.gz\n",
            "Unzipped: sets.csv.gz\n",
            "Unzipped: minifigs.csv.gz\n",
            "Unzipped: part_categories.csv.gz\n",
            "Unzipped: parts.csv.gz\n",
            "Unzipped: colors.csv.gz\n",
            "Unzipped: inventory_parts.csv.gz\n",
            "Unzipped: inventory_minifigs.csv.gz\n",
            "Unzipped: part_relationships.csv.gz\n",
            "Unzipped: elements.csv.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy pathlib sentence-transformers pymilvus pillow transformers torch gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3TzvLPv0iWm",
        "outputId": "4516913d-f72e-4119-ea3c-2be1aa291047"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.3.1)\n",
            "Requirement already satisfied: pymilvus in /usr/local/lib/python3.11/dist-packages (2.5.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.13.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.27.1)\n",
            "Requirement already satisfied: setuptools>69 in /usr/local/lib/python3.11/dist-packages (from pymilvus) (75.1.0)\n",
            "Requirement already satisfied: grpcio<=1.67.1,>=1.49.1 in /usr/local/lib/python3.11/dist-packages (from pymilvus) (1.67.1)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.11/dist-packages (from pymilvus) (4.25.5)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from pymilvus) (1.0.1)\n",
            "Requirement already satisfied: ujson>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pymilvus) (5.10.0)\n",
            "Requirement already satisfied: milvus-lite>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from pymilvus) (2.4.11)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.7)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.6.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.6.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.5)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.9.3)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.45.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.6.0->gradio) (14.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Dependencies\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType\n",
        "import logging\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, CLIPModel, AutoModelForCausalLM, AutoTokenizer\n",
        "import gradio as gr\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "metadata": {
        "id": "b3Do5Xrt0Z3k"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 0: Drop and rebuild\n",
        "# Add this method to LegoVectorDB\n",
        "def _init_collection(self):\n",
        "    # Drop existing collection if it exists\n",
        "    if Collection.loaded():\n",
        "        Collection.drop_collection(\"lego_sets\")\n",
        "\n",
        "    fields = [\n",
        "        FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True),\n",
        "        FieldSchema(name=\"set_num\", dtype=DataType.VARCHAR, max_length=20),\n",
        "        FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=384),\n",
        "        FieldSchema(name=\"parts_description\", dtype=DataType.VARCHAR, max_length=2000)\n",
        "    ]\n",
        "    schema = CollectionSchema(fields=fields, description=\"LEGO sets\")\n",
        "    collection = Collection(name=\"lego_sets\", schema=schema)\n",
        "    collection.create_index(\n",
        "    field_name=\"embedding\",\n",
        "    index_params={\"index_type\": \"IVF_FLAT\", \"metric_type\": \"COSINE\", \"params\": {\"nlist\": 128}}\n",
        ")\n",
        "    return collection"
      ],
      "metadata": {
        "id": "Rv7_RzUV0eFU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Data Access\n",
        "class LegoDatabase:\n",
        "    def __init__(self, data_dir='women-in-ai-hackathon/data'):\n",
        "        self.data_path = Path(data_dir)\n",
        "        self.dfs = self._load_data()\n",
        "\n",
        "    def _load_data(self):\n",
        "      return {\n",
        "          'parts': pd.read_csv(self.data_path / 'parts.csv.csv'),\n",
        "          'sets': pd.read_csv(self.data_path / 'sets.csv.csv'),\n",
        "          'inventory_parts': pd.read_csv(self.data_path / 'inventory_parts.csv.csv'),\n",
        "          'inventories': pd.read_csv(self.data_path / 'inventories.csv.csv'),\n",
        "          'colors': pd.read_csv(self.data_path / 'colors.csv.csv'),\n",
        "          'themes': pd.read_csv(self.data_path / 'themes.csv.csv')\n",
        "      }\n",
        "\n",
        "    def get_set_parts(self, set_num):\n",
        "      # Get set info\n",
        "      set_info = self.dfs['sets'][self.dfs['sets']['set_num'] == set_num].iloc[0]\n",
        "\n",
        "      # Get inventories for the set\n",
        "      inventories = self.dfs['inventories'][self.dfs['inventories']['set_num'] == set_num]['id']\n",
        "\n",
        "      # Get parts for those inventories\n",
        "      parts = self.dfs['inventory_parts'][self.dfs['inventory_parts']['inventory_id'].isin(inventories)]\n",
        "\n",
        "      # Basic merge with parts table\n",
        "      parts = parts.merge(self.dfs['parts'][['part_num', 'name']], on='part_num')\n",
        "\n",
        "      print(\"Parts columns after merge:\", parts.columns.tolist())  # Temporary debug line\n",
        "\n",
        "      return parts, set_info\n",
        "\n"
      ],
      "metadata": {
        "id": "zoYtrQqp0f5u"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: VectorDB\n",
        "class LegoVectorDB:\n",
        "    def __init__(self, uri, token):\n",
        "        connections.connect(uri=uri, token=token)\n",
        "        self.collection = self._init_collection()\n",
        "\n",
        "    def _init_collection(self):\n",
        "        from pymilvus import utility\n",
        "\n",
        "        # Delete existing collection if it exists\n",
        "        if utility.has_collection(\"lego_sets\"):\n",
        "            utility.drop_collection(\"lego_sets\")\n",
        "\n",
        "        fields = [\n",
        "            FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True),\n",
        "            FieldSchema(name=\"set_num\", dtype=DataType.VARCHAR, max_length=20),\n",
        "            FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=384),  # Match model output dimension\n",
        "            FieldSchema(name=\"parts_description\", dtype=DataType.VARCHAR, max_length=2000)\n",
        "        ]\n",
        "        schema = CollectionSchema(fields=fields, description=\"LEGO sets\")\n",
        "        collection = Collection(name=\"lego_sets\", schema=schema)\n",
        "        collection.create_index(\n",
        "            field_name=\"embedding\",\n",
        "            index_params={\"index_type\": \"IVF_FLAT\", \"metric_type\": \"L2\", \"params\": {\"nlist\": 128}}\n",
        "        )\n",
        "        return collection"
      ],
      "metadata": {
        "id": "BP7fmZcF0xg6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: RAG\n",
        "class LegoRAG:\n",
        "    def __init__(self, uri, token):\n",
        "        self.db = LegoDatabase()\n",
        "        self.vector_db = LegoVectorDB(uri=uri, token=token)\n",
        "        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        self.id_counter = 0\n",
        "\n",
        "    def encode_set(self, parts_df):\n",
        "      parts_text = \" \".join([f\"{row['quantity']} x {row['name']}\"\n",
        "                          for _, row in parts_df.iterrows()])\n",
        "      embedding = self.encoder.encode(parts_text)\n",
        "      return embedding, parts_text\n",
        "\n",
        "    def index_set(self, set_num):\n",
        "      parts, set_info = self.db.get_set_parts(set_num)\n",
        "      if len(parts) > 0:\n",
        "          embedding, description = self.encode_set(parts)\n",
        "          self.vector_db.collection.insert([\n",
        "              [self.id_counter],\n",
        "              [set_num],\n",
        "              [embedding.tolist()],\n",
        "              [description]\n",
        "          ])\n",
        "          self.id_counter += 1\n",
        "          return True\n",
        "      return False\n",
        "\n",
        "    def find_similar(self, set_num, top_k=5):\n",
        "      parts, _ = self.db.get_set_parts(set_num)\n",
        "      query_embedding, _ = self.encode_set(parts)\n",
        "      return self.vector_db.collection.search(\n",
        "          data=[query_embedding.tolist()],\n",
        "          anns_field=\"embedding\",\n",
        "          param={\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 10}},\n",
        "          limit=top_k,\n",
        "          output_fields=[\"set_num\", \"parts_description\"])"
      ],
      "metadata": {
        "id": "w68YNddK0xfV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Creating Interface\n",
        "def create_interface(rag):\n",
        "    def process_query(set_num):\n",
        "        try:\n",
        "            parts, set_info = rag.db.get_set_parts(set_num)\n",
        "            if parts.empty:\n",
        "                return \"Error: Set number not found\"\n",
        "\n",
        "            theme = rag.db.dfs['themes'][rag.db.dfs['themes']['id'] == set_info['theme_id']].iloc[0]['name']\n",
        "            output = [f\"Query Set: {set_num}\\nTheme: {theme}\\nYear: {set_info['year']}\\nPieces: {set_info['num_parts']}\\n\\nSimilar Sets:\\n\"]\n",
        "\n",
        "            results = rag.find_similar(set_num)\n",
        "            for hit in results[0]:\n",
        "                similar_set_num = hit.entity.get('set_num')\n",
        "                similar_set = rag.db.dfs['sets'][rag.db.dfs['sets']['set_num'] == similar_set_num].iloc[0]\n",
        "                similar_theme = rag.db.dfs['themes'][rag.db.dfs['themes']['id'] == similar_set['theme_id']].iloc[0]['name']\n",
        "\n",
        "                output.append(f\"Set: {similar_set_num}\\n\"\n",
        "                            f\"Theme: {similar_theme}\\n\"\n",
        "                            f\"Year: {similar_set['year']}\\n\"\n",
        "                            f\"Similarity: {hit.score:.2f}\\n\"\n",
        "                            f\"Parts: {hit.entity.get('parts_description')}\\n\")\n",
        "            return \"\\n\".join(output)\n",
        "        except Exception as e:\n",
        "            return f\"Error: {str(e)}\"\n",
        "\n",
        "    interface = gr.Interface(\n",
        "        fn=process_query,\n",
        "        inputs=gr.Textbox(label=\"Enter LEGO Set Number\"),\n",
        "        outputs=gr.Textbox(label=\"Similar Sets\"),\n",
        "        title=\"Brickspiration\"\n",
        "    )\n",
        "    return interface"
      ],
      "metadata": {
        "id": "MvXava-E0xc4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Playground\n",
        "if __name__ == \"__main__\":\n",
        "    rag = LegoRAG(\n",
        "        uri=\"\",\n",
        "        token=\"\"\n",
        "    )\n",
        "\n",
        "    # Index sample sets\n",
        "    sample_sets = rag.db.dfs['sets']['set_num'].head(10).tolist()\n",
        "    for set_num in sample_sets:\n",
        "        if rag.index_set(set_num):\n",
        "            print(f\"Indexed set {set_num}\")\n",
        "\n",
        "    # Launch interface\n",
        "    interface = create_interface(rag)\n",
        "    interface.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 934
        },
        "id": "lpXeM59H0xaq",
        "outputId": "f429798a-71f9-4eb9-84a0-0601c808effa"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parts columns after merge: ['inventory_id', 'part_num', 'color_id', 'quantity', 'is_spare', 'img_url', 'name']\n",
            "Indexed set 0003977811-1\n",
            "Parts columns after merge: ['inventory_id', 'part_num', 'color_id', 'quantity', 'is_spare', 'img_url', 'name']\n",
            "Indexed set 001-1\n",
            "Parts columns after merge: ['inventory_id', 'part_num', 'color_id', 'quantity', 'is_spare', 'img_url', 'name']\n",
            "Parts columns after merge: ['inventory_id', 'part_num', 'color_id', 'quantity', 'is_spare', 'img_url', 'name']\n",
            "Parts columns after merge: ['inventory_id', 'part_num', 'color_id', 'quantity', 'is_spare', 'img_url', 'name']\n",
            "Indexed set 0012-1\n",
            "Parts columns after merge: ['inventory_id', 'part_num', 'color_id', 'quantity', 'is_spare', 'img_url', 'name']\n",
            "Indexed set 0013-1\n",
            "Parts columns after merge: ['inventory_id', 'part_num', 'color_id', 'quantity', 'is_spare', 'img_url', 'name']\n",
            "Indexed set 0014-1\n",
            "Parts columns after merge: ['inventory_id', 'part_num', 'color_id', 'quantity', 'is_spare', 'img_url', 'name']\n",
            "Indexed set 0015-1\n",
            "Parts columns after merge: ['inventory_id', 'part_num', 'color_id', 'quantity', 'is_spare', 'img_url', 'name']\n",
            "Indexed set 0016-1\n",
            "Parts columns after merge: ['inventory_id', 'part_num', 'color_id', 'quantity', 'is_spare', 'img_url', 'name']\n",
            "Indexed set 002-1\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://b02ac9d33553761f63.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b02ac9d33553761f63.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xfBKWUWm0xYm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Alternate Idea\n",
        "import torch\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType\n",
        "\n",
        "class LegoSemanticSearch:\n",
        "    def __init__(self, data_dir='women-in-ai-hackathon/data',\n",
        "                 uri=\"\",\n",
        "                 token=\"\"):\n",
        "        # Load data\n",
        "        self.data_path = Path(data_dir)\n",
        "        self.dfs = self._load_data()\n",
        "\n",
        "        # Initialize embedding model\n",
        "        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "        # Connect to Milvus vector database\n",
        "        connections.connect(uri=uri, token=token)\n",
        "        self.collection = self._init_collection()\n",
        "\n",
        "        # ID counter for vector database\n",
        "        self.id_counter = 0\n",
        "\n",
        "    def _load_data(self):\n",
        "        files = ['parts', 'sets', 'inventory_parts', 'inventories', 'colors', 'themes']\n",
        "        return {f: pd.read_csv(self.data_path / f'{f}.csv.csv') for f in files}\n",
        "\n",
        "    def _init_collection(self):\n",
        "        from pymilvus import utility\n",
        "\n",
        "        if utility.has_collection(\"lego_semantic_sets\"):\n",
        "            utility.drop_collection(\"lego_semantic_sets\")\n",
        "\n",
        "        fields = [\n",
        "            FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True),\n",
        "            FieldSchema(name=\"set_num\", dtype=DataType.VARCHAR, max_length=20),\n",
        "            FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=384),\n",
        "            FieldSchema(name=\"set_description\", dtype=DataType.VARCHAR, max_length=2000)\n",
        "        ]\n",
        "        schema = CollectionSchema(fields=fields, description=\"LEGO sets semantic search\")\n",
        "        collection = Collection(name=\"lego_semantic_sets\", schema=schema)\n",
        "        collection.create_index(\n",
        "            field_name=\"embedding\",\n",
        "            index_params={\n",
        "                \"index_type\": \"IVF_FLAT\",\n",
        "                \"metric_type\": \"COSINE\",\n",
        "                \"params\": {\"nlist\": 128}\n",
        "            }\n",
        "        )\n",
        "        collection.load()\n",
        "        return collection\n",
        "\n",
        "    def create_set_description(self, set_row):\n",
        "        \"\"\"\n",
        "        Create a descriptive text for a LEGO set\n",
        "        \"\"\"\n",
        "        theme = self.dfs['themes'][self.dfs['themes']['id'] == set_row['theme_id']].iloc[0]['name']\n",
        "        return f\"{set_row['name']} - {theme} set from {set_row['year']} (Pieces: {set_row['num_parts']})\"\n",
        "\n",
        "    def index_sets(self, limit=None):\n",
        "        \"\"\"\n",
        "        Index LEGO sets into vector database\n",
        "        \"\"\"\n",
        "        sets_df = self.dfs['sets']\n",
        "        if limit:\n",
        "            sets_df = sets_df.head(limit)\n",
        "\n",
        "        for _, set_row in sets_df.iterrows():\n",
        "            description = self.create_set_description(set_row)\n",
        "            embedding = self.encoder.encode(description)\n",
        "\n",
        "            self.collection.insert([\n",
        "                [self.id_counter],\n",
        "                [set_row['set_num']],\n",
        "                [embedding.tolist()],\n",
        "                [description]\n",
        "            ])\n",
        "            self.id_counter += 1\n",
        "\n",
        "        self.collection.flush()\n",
        "\n",
        "    def semantic_search(self, query, top_k=5):\n",
        "        \"\"\"\n",
        "        Perform semantic search on LEGO sets\n",
        "        \"\"\"\n",
        "        # Encode query\n",
        "        query_embedding = self.encoder.encode(query)\n",
        "\n",
        "        # Search in vector database\n",
        "        results = self.collection.search(\n",
        "            data=[query_embedding.tolist()],\n",
        "            anns_field=\"embedding\",\n",
        "            param={\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 10}},\n",
        "            limit=top_k,\n",
        "            output_fields=[\"set_num\", \"set_description\"]\n",
        "        )\n",
        "\n",
        "        # Process and return results\n",
        "        return [\n",
        "            {\n",
        "                'set_num': hit.entity.get('set_num'),\n",
        "                'description': hit.entity.get('set_description'),\n",
        "                'similarity': hit.score\n",
        "            }\n",
        "            for hit in results[0]\n",
        "        ]\n",
        "\n",
        "# Example usage\n",
        "def main():\n",
        "    # Initialize Lego Semantic Search\n",
        "    lego_search = LegoSemanticSearch()\n",
        "\n",
        "    # Index sets\n",
        "    lego_search.index_sets(limit=100)\n",
        "\n",
        "    # Perform semantic searches\n",
        "    queries = [\n",
        "        \"Star Wars spaceship\",\n",
        "        \"Castle medieval\",\n",
        "        \"Technic vehicle\",\n",
        "        \"Space exploration\"\n",
        "    ]\n",
        "\n",
        "    for query in queries:\n",
        "        print(f\"\\nSearch Query: {query}\")\n",
        "        results = lego_search.semantic_search(query)\n",
        "\n",
        "        for result in results:\n",
        "            print(f\"Set: {result['set_num']} | Description: {result['description']} | Similarity: {result['similarity']:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAKJI_Gs0xWn",
        "outputId": "228b9dd7-1345-43d0-be4b-b98fdb7fbdac"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Search Query: Star Wars spaceship\n",
            "Set: 0241357594-1 | Description: Star Wars: Build Your Own Adventure: Galactic Missions - Activity Books with LEGO Parts set from 2019 (Pieces: 70) | Similarity: 0.4674\n",
            "Set: 0878119001641-1 | Description: Star Wars Battle Bridge storage case - Storage set from 2012 (Pieces: 0) | Similarity: 0.4055\n",
            "Set: 1000368666-1 | Description: Batman the Videogame - PSP - Video Games and Accessories set from 2008 (Pieces: 0) | Similarity: 0.2974\n",
            "Set: 1000430096-8 | Description: Marvel Super Heroes - PS4 - Video Games and Accessories set from 2013 (Pieces: 0) | Similarity: 0.2855\n",
            "Set: 0015-1 | Description: Space Mini-Figures - Supplemental set from 1979 (Pieces: 18) | Similarity: 0.2855\n",
            "\n",
            "Search Query: Castle medieval\n",
            "Set: 0016-1 | Description: Castle Mini Figures - Classic Castle set from 1979 (Pieces: 15) | Similarity: 0.4971\n",
            "Set: 0011-3 | Description: Castle 2 for 1 Bonus Offer - Lion Knights set from 1987 (Pieces: 0) | Similarity: 0.4387\n",
            "Set: 10000-1 | Description: Guarded Inn - Lion Knights set from 2001 (Pieces: 256) | Similarity: 0.4226\n",
            "Set: 028-1 | Description: Nursery Furniture - Playhouse set from 1979 (Pieces: 7) | Similarity: 0.3786\n",
            "Set: 041-2 | Description: Playhouse - Playhouse set from 1979 (Pieces: 1) | Similarity: 0.3632\n",
            "\n",
            "Search Query: Technic vehicle\n",
            "Set: 06-1 | Description: Motor Wires - Technic set from 1980 (Pieces: 6) | Similarity: 0.4648\n",
            "Set: 02-1 | Description: Extra Large Tires & Hubs - Technic set from 1982 (Pieces: 4) | Similarity: 0.4152\n",
            "Set: 01-1 | Description: Chain Links - Technic set from 1980 (Pieces: 25) | Similarity: 0.4004\n",
            "Set: 01-2 | Description: Bulldozer Chain Links - Technic set from 1982 (Pieces: 50) | Similarity: 0.3736\n",
            "Set: 07-1 | Description: Battery Box - Technic set from 1980 (Pieces: 1) | Similarity: 0.3669\n",
            "\n",
            "Search Query: Space exploration\n",
            "Set: 0241357594-1 | Description: Star Wars: Build Your Own Adventure: Galactic Missions - Activity Books with LEGO Parts set from 2019 (Pieces: 70) | Similarity: 0.3983\n",
            "Set: 0756668530-1 | Description: Atlantis: Brickmaster - Activity Books with LEGO Parts set from 2010 (Pieces: 157) | Similarity: 0.2590\n",
            "Set: 0014-1 | Description: Space Mini-Figures - Supplemental set from 1979 (Pieces: 2) | Similarity: 0.2379\n",
            "Set: 03093-1 | Description: The Race to Build It Board Game - Early Creator set from 1999 (Pieces: 77) | Similarity: 0.2352\n",
            "Set: 0015-1 | Description: Space Mini-Figures - Supplemental set from 1979 (Pieces: 18) | Similarity: 0.2346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PQcWyzlK0xUR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qGmCnwMS0xSH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GLZTVJnx0xPu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DxxP03Km0xAX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}